{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pJkBmp4ENBv",
        "outputId": "33ff1c01-dd8b-409c-cc30-a8ebc9927a93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the website URL to scrape: https://www.bbc.com/news\n",
            "Data saved to scraped_data.csv\n",
            "Data saved to scraped_data.json\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import json\n",
        "\n",
        "def scrape_website(url):\n",
        "    \"\"\"Scrape headlines and links from the given website.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching the webpage: {e}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Find all links and their text (Modify this according to your target website)\n",
        "    articles = soup.find_all(\"a\")\n",
        "\n",
        "    data = []\n",
        "    for article in articles:\n",
        "        title = article.get_text(strip=True)\n",
        "        link = article.get(\"href\", \"\")\n",
        "\n",
        "        # Skip empty titles\n",
        "        if title:\n",
        "            full_link = link if link.startswith(\"http\") else f\"{url.rstrip('/')}/{link.lstrip('/')}\"\n",
        "            data.append({\"title\": title, \"link\": full_link})\n",
        "\n",
        "    return data\n",
        "\n",
        "def save_to_csv(data, filename=\"scraped_data.csv\"):\n",
        "    \"\"\"Save scraped data to a CSV file.\"\"\"\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\"title\", \"link\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def save_to_json(data, filename=\"scraped_data.json\"):\n",
        "    \"\"\"Save scraped data to a JSON file.\"\"\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
        "        json.dump(data, file, indent=4)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    user_url = input(\"Enter the website URL to scrape: \")\n",
        "    scraped_data = scrape_website(user_url)\n",
        "\n",
        "    if scraped_data:\n",
        "        save_to_csv(scraped_data)\n",
        "        save_to_json(scraped_data)\n",
        "    else:\n",
        "        print(\"No data found or invalid website.\")\n"
      ]
    }
  ]
}